// import { Worker } from "bullmq";
// import axios from "axios";
// import xml2js from "xml2js";
// import { redis } from "../config/redis.js";
// import mongoose from "mongoose";
// import dotenv from "dotenv";
// import crypto from "crypto";
// import Job from "../models/job.js";
// import ImportLog from "../models/ImportLog.js";

// dotenv.config();

// // âœ… Connect to MongoDB
// mongoose.connect(process.env.MONGO_URI, {
//   useNewUrlParser: true,
//   useUnifiedTopology: true,
// });

// // âœ… Worker for job-import-queue
// const worker = new Worker(
//   "job-import-queue",
//   async (job) => {
//     const { feedUrl, fileName } = job.data;
//     console.log(`âš™ï¸ Processing: ${fileName} â€” ${feedUrl}`);

//     const startTime = Date.now();
//     let totalFetched = 0,
//       totalImported = 0,
//       newJobs = 0,
//       updatedJobs = 0,
//       failedJobs = [];

//     try {
//       // 1ï¸âƒ£ Fetch XML
//       const { data } = await axios.get(feedUrl, { timeout: 15000 });

//       // 2ï¸âƒ£ Convert XML â†’ JSON
//       const parsed = await xml2js.parseStringPromise(data, {
//         explicitArray: false,
//       });

//       // 3ï¸âƒ£ Extract job list
//       const items = parsed.rss?.channel?.item || [];
//       const jobs = Array.isArray(items) ? items : [items];
//       totalFetched = jobs.length;

//       // 4ï¸âƒ£ Process each job
//       for (const j of jobs) {
//         try {
//           // âœ… Generate unique externalId
//           const externalId =
//             j.guid ||
//             j.id ||
//             (j.link
//               ? crypto.createHash("md5").update(j.link).digest("hex")
//               : crypto
//                   .createHash("md5")
//                   .update(j.title + "_" + (j.pubDate || ""))
//                   .digest("hex"));

//           // âœ… Generate unique jobId fallback to avoid duplicate key errors
//           const jobId =
//             j.jobId ||
//             j.guid ||
//             crypto
//               .createHash("md5")
//               .update(j.link || j.title)
//               .digest("hex");

//           // âœ… Construct payload only with schema fields
//           const payload = {
//             externalId,
//             jobId,
//             title: j.title || "",
//             company: j["company"] || j["author"] || j["dc:creator"] || "",
//             location: j["location"] || "",
//             description: j["description"] || "",
//             url: j.link || "",
//             category: j["category"] || "",
//             jobType: j["jobType"] || "",
//             salary: j["salary"] || "",
//             postedAt: j.pubDate ? new Date(j.pubDate) : new Date(),
//             raw: j,
//           };

//           // âœ… Upsert safely using externalId
//           const result = await Job.updateOne(
//             { externalId },
//             { $set: payload },
//             { upsert: true }
//           );

//           if (result.upsertedCount) newJobs++;
//           else updatedJobs++;

//           totalImported++;
//         } catch (err) {
//           failedJobs.push({
//             externalId: j.guid || j.id || j.link || j.title,
//             reason: err.message,
//           });
//         }
//       }

//       // 5ï¸âƒ£ Log import results
//       const durationMs = Date.now() - startTime;
//       await ImportLog.create({
//         feedUrl,
//         fileName,
//         totalFetched,
//         totalImported,
//         newJobs,
//         updatedJobs,
//         failedJobs,
//         durationMs,
//       });

//       console.log(
//         `âœ… Done ${fileName}: fetched ${totalFetched}, imported ${totalImported} (${newJobs} new, ${updatedJobs} updated)`
//       );
//     } catch (err) {
//       console.error(`âŒ Import failed for ${fileName}:`, err.message);
//     }
//   },
//   { connection: redis, concurrency: 5 }
// );

// // âœ… Event listeners
// worker.on("completed", (job) =>
//   console.log(`ğŸ‰ Job ${job.id} completed successfully`)
// );
// worker.on("failed", (job, err) =>
//   console.error(`ğŸ’¥ Job ${job.id} failed: ${err.message}`)
// );

import { Worker } from "bullmq";
import axios from "axios";
import xml2js from "xml2js";
import { redis } from "../config/redis.js";
import mongoose from "mongoose";
import dotenv from "dotenv";
import crypto from "crypto";
import Job from "../models/job.js";
import ImportLog from "../models/ImportLog.js";

dotenv.config();

// âœ… Connect to MongoDB
mongoose.connect(process.env.MONGO_URI, {
  useNewUrlParser: true,
  useUnifiedTopology: true,
});

// âœ… Worker for job-import-queue
const worker = new Worker(
  "job-import-queue",
  async (job) => {
    const { feedUrl, fileName } = job.data;
    console.log(`âš™ï¸ Processing feed: ${fileName} â€” ${feedUrl}`);

    const startTime = Date.now();
    let totalFetched = 0;
    let newJobs = 0;
    let updatedJobs = 0;
    let failedJobs = [];

    try {
      // 1ï¸âƒ£ Fetch XML
      const { data } = await axios.get(feedUrl, { timeout: 15000 });

      // 2ï¸âƒ£ Convert XML â†’ JSON
      const parsed = await xml2js.parseStringPromise(data, {
        explicitArray: false,
      });

      // 3ï¸âƒ£ Extract job list
      const items = parsed.rss?.channel?.item || [];
      let jobs = Array.isArray(items) ? items : [items];
      totalFetched = jobs.length;

      // 4ï¸âƒ£ Deduplicate jobs within the feed (based on guid/id/link/title)
      const seen = new Set();
      jobs = jobs.filter((j) => {
        const id = j.guid || j.id || j.link || j.title;
        if (seen.has(id)) return false;
        seen.add(id);
        return true;
      });
      totalFetched = jobs.length; // update after deduplication

      // 5ï¸âƒ£ Process each job
      for (const j of jobs) {
        try {
          // âœ… Generate unique externalId
          const externalId =
            j.guid ||
            j.id ||
            (j.link
              ? crypto.createHash("md5").update(j.link).digest("hex")
              : crypto
                  .createHash("md5")
                  .update(j.title + "_" + (j.pubDate || ""))
                  .digest("hex"));

          // âœ… Construct payload
          const payload = {
            externalId,
            title: j.title || "",
            company: j["company"] || j["author"] || j["dc:creator"] || "",
            location: j["location"] || "",
            description: j["description"] || "",
            url: j.link || "",
            category: j["category"] || "",
            jobType: j["jobType"] || "",
            salary: j["salary"] || "",
            postedAt: j.pubDate ? new Date(j.pubDate) : new Date(),
            raw: j,
          };

          // âœ… Upsert safely using externalId
          const result = await Job.updateOne(
            { externalId },
            { $set: payload },
            { upsert: true }
          );

          if (result.upsertedCount > 0) newJobs++;
          else updatedJobs++;
        } catch (err) {
          failedJobs.push({
            externalId: j.guid || j.id || j.link || j.title,
            reason: err.message,
          });
        }
      }

      // 6ï¸âƒ£ Log **one entry per feed**
      await ImportLog.create({
        feedUrl,
        fileName,
        totalFetched,
        totalImported: newJobs + updatedJobs,
        newJobs,
        updatedJobs,
        failedJobs, // array of failed jobs
        durationMs: Date.now() - startTime,
      });

      console.log(
        `âœ… Done ${fileName}: Fetched=${totalFetched}, Imported=${
          newJobs + updatedJobs
        } (New=${newJobs}, Updated=${updatedJobs}, Failed=${failedJobs.length})`
      );
    } catch (err) {
      console.error(`âŒ Feed import failed for ${fileName}:`, err.message);

      // Log even if feed fails completely
      await ImportLog.create({
        feedUrl,
        fileName,
        totalFetched: 0,
        totalImported: 0,
        newJobs: 0,
        updatedJobs: 0,
        failedJobs: [{ reason: err.message }],
        durationMs: Date.now() - startTime,
      });
    }
  },
  { connection: redis, concurrency: 5 }
);

// âœ… Event listeners
worker.on("completed", (job) =>
  console.log(`ğŸ‰ Job ${job.id} completed successfully`)
);
worker.on("failed", (job, err) =>
  console.error(`ğŸ’¥ Job ${job.id} failed: ${err.message}`)
);

export default worker;
